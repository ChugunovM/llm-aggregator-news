import logging
import time
from datetime import datetime
from typing import List, Optional
from celery import Task
from app.celery_app import celery_app
from app.reddit_client import get_reddit_client
from app.config import settings

logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –æ–±—Ä–µ–∑–∫–∞ –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    if not text:
        return ""
    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
    text = " ".join(text.split())
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (LLM –Ω–µ –ª—é–±–∏—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –≤—Ö–æ–¥—ã)
    return text[:2000]

@celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
def scrape_reddit_for_company(
    self: Task,
    company_name: str,
    subreddits: Optional[List[str]] = None,
    limit_per_sub: int = 10,
    time_filter: str = "week"
) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç Reddit –≤ –ø–æ–∏—Å–∫–∞—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π.

    Args:
        company_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Tesla")
        subreddits: —Å–ø–∏—Å–æ–∫ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä ["stocks", "technology", "teslamotors"]
                    –µ—Å–ª–∏ None - –∏—â–µ–º –≤ r/all
        limit_per_sub: —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –±—Ä–∞—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–∞
        time_filter: –∑–∞ –∫–∞–∫–æ–π –ø–µ—Ä–∏–æ–¥ –∏—Å–∫–∞—Ç—å
    """
    logger.info(f"üîç Starting Reddit scrape for '{company_name}' in {subreddits or ['all']}")

    try:
        reddit = get_reddit_client()
        results = []

        targets = subreddits if subreddits else ["all"]

        for subreddit_name in targets:
            try:
                subreddit = reddit.subreddit(subreddit_name)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º search –≤–º–µ—Å—Ç–æ hot/new - –∏—â–µ–º –∏–º–µ–Ω–Ω–æ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
                submissions = subreddit.search(
                    query=f'"{company_name}"',
                    sort="new",
                    limit=limit_per_sub,
                    time_filter=time_filter
                )

                for post in submissions:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º sticky-–ø–æ—Å—Ç—ã –∏ —É–¥–∞–ª—ë–Ω–Ω—ã–µ
                    if post.stickied or post.removed_by_category:
                        continue

                    item = {
                        "source": "reddit",
                        "company": company_name,
                        "title": clean_text(post.title),
                        "text": clean_text(post.selftext),
                        "url": f"https://reddit.com{post.permalink}",
                        "author": str(post.author) if post.author else "[deleted]",
                        "subreddit": subreddit_name,
                        "score": post.score,
                        "created_utc": datetime.utcfromtimestamp(post.created_utc).isoformat(),
                        "raw_id": post.id,
                    }
                    results.append(item)
                
                # –í–µ–∂–ª–∏–≤–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (Reddit –º–æ–∂–µ—Ç –±–∞–Ω–∏—Ç—å –∑–∞ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
                time.sleep(1)
            
            except Exception as sub_e:
                logger.warning(f"‚ö†Ô∏è Error scraping r/{subreddit_name}: {sub_e}")
                continue # –Ω–µ –ø–∞–¥–∞–µ–º –∏–∑-–∑–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–∞
        
        logger.info(f"‚úÖ Found {len(results)} Reddit posts for '{company_name}'")

        # TODO: –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ LLM-–æ–±—Ä–∞–±–æ—Ç–∫—É
        # from .llm_task import process_raw_item
        # for item in results:
        #       process_raw_item.delay(item)

        return {
            "company": company_name,
            "source": "reddit",
            "posts_found": len(results),
            "sample_urls": [r["url"] for r in results[:3]],
            "status": "success"
        }

    except Exception as exc:
        logger.error(f"‚ùå Fatal error in Reddit scraping: {exc}")
        raise self.retry(exc=exc)